# Booster K1 Project Structure

**Last Updated:** 2025-11-19
**Architecture:** Hybrid ROS2 + Booster SDK with Fleet Coordination

---

## ğŸ“‹ Quick Reference

| Category | Files | Status |
|----------|-------|--------|
| **Core Control** | 1 Python | âœ… Production |
| **Vision** | 5 Python | âœ… Production (3D depth added) |
| **Fleet Ops** | 2 Python | ğŸ†• New (needs Gun.js server) |
| **Testing** | 4 Scripts | âœ… Production |
| **Deployment** | 3 Scripts | âœ… Production |
| **Documentation** | 5 Markdown | âœ… Complete |

---

## ğŸ¯ Core Robotics

### `src/basic_controls.py` âœ… PRODUCTION
**Purpose:** Low-level robot control via Booster SDK
**Status:** Production-ready with emergency stop
**Dependencies:**
- `booster_robotics_sdk_python` (B1LocoClient, ChannelFactory)
- `src/voice_controller.py` (optional, import attempted)

**Key Features:**
- SDK connection via `127.0.0.1` (localhost)
- WASD keyboard teleoperation (50 Hz control loop)
- Emergency stop system (atexit + signal handlers)
- Mode control (Damping/Prepare/Walking/Custom)
- Hand control (rock/paper/scissors, grasp, OK gestures)
- Head control (up/down/left/right)
- Voice commands (optional integration)

**Usage:**
```bash
python src/basic_controls.py              # Uses 127.0.0.1 by default
python src/basic_controls.py 192.168.1.5  # Override network interface
```

**Network Architecture:**
```
basic_controls.py â†’ ChannelFactory(127.0.0.1) â†’ K1 SDK Daemon â†’ Hardware
```

---

## ğŸ“¹ Computer Vision

### `src/basic_cam.py` âœ… PRODUCTION
**Purpose:** Minimal camera feed viewer (no detection)
**Status:** Production
**Dependencies:**
- ROS2 (`rclpy`, `sensor_msgs.msg.Image`)
- `cv_bridge`, `cv2`

**Key Features:**
- Subscribes to `/booster_camera_bridge/image_left_raw` and `image_right_raw`
- NV12 â†’ BGR conversion for ZED camera
- Web interface on port 8080
- Stereo support with `--stereo` flag

**ROS2 Topics:**
- **Subscribe:** `/booster_camera_bridge/image_left_raw`, `/image_right_raw`
- **Publish:** None (HTTP only)

---

### `src/came_yolo.py` ğŸ†• PRODUCTION (3D DEPTH ADDED)
**Purpose:** YOLO object detection with 3D depth integration
**Status:** Production with TensorRT support
**Dependencies:**
- ROS2 (`rclpy`, `sensor_msgs.msg.Image`)
- `cv_bridge`, `cv2`, `numpy`
- `ultralytics` (YOLO)

**Key Features:**
- **3D object localization** using depth camera
- **TensorRT optimization** (3-7x speedup on Jetson)
- Subscribes to RGB + depth streams
- Calculates 3D positions (x, y, z) from depth data
- Web interface with depth overlay
- `detector.get_detections_3d()` API

**ROS2 Topics:**
- **Subscribe:** `/booster_camera_bridge/image_left_raw`, `/depth_raw`
- **Publish:** None (HTTP only)

**Usage:**
```bash
python src/came_yolo.py                           # Standard mode
python src/came_yolo.py --tensorrt                # Use TensorRT engine
python src/came_yolo.py --model yolov8s --depth   # Larger model with 3D
python src/came_yolo.py --no-depth                # Disable depth
```

**Data Flow:**
```
RGB topic â†’ YOLO detection â†’ Bounding boxes
Depth topic â†’ Depth lookup â†’ 3D positions (x,y,z)
Combined â†’ Web UI display
```

---

### `src/cam_face_recognition.py` âœ… PRODUCTION
**Purpose:** Face recognition with database
**Status:** Production
**Dependencies:**
- ROS2, `cv_bridge`, `cv2`
- `src/face_recognition.py` (FaceRecognizer)
- `src/tts_module.py` (TextToSpeech)

**Key Features:**
- DeepFace-based face recognition
- Person database (JSON)
- TTS announcements
- Web interface

**ROS2 Topics:**
- **Subscribe:** `/booster_camera_bridge/image_left_raw`

---

### `src/smart_recognition.py` âœ… PRODUCTION
**Purpose:** Unified recognition system (YOLO + faces + voice learning)
**Status:** Production with conversation recording
**Dependencies:**
- ROS2, `cv_bridge`, `cv2`
- `src/face_recognition.py`
- `src/tts_module.py`
- `src/voice_listener.py`
- `ultralytics` (YOLO)

**Key Features:**
- YOLO object detection
- Face recognition
- Voice-based learning ("What's your name?")
- Conversation recording
- Person database with conversation history
- Web interface + voice interaction

**Database Schema:**
```json
{
  "people": {
    "John": {
      "first_seen": "timestamp",
      "last_seen": "timestamp",
      "times_seen": 5,
      "conversations": ["I'm here to...", "..."]
    }
  },
  "objects": {...},
  "opt_out": ["privacy-conscious-person"]
}
```

---

### `src/face_recognition.py` âœ… LIBRARY
**Purpose:** Face recognition module (DeepFace wrapper)
**Status:** Production library
**Dependencies:**
- `deepface` (optional, falls back to OpenCV)
- `cv2`

**API:**
```python
recognizer = FaceRecognizer(use_deepface=True)
faces = recognizer.detect_faces(frame)
match = recognizer.recognize_face(face_img, database)
```

---

## ğŸ¤– Fleet Operations (NEW)

### `src/booster_ros2_bridge.py` ğŸ†• PRODUCTION
**Purpose:** ROS2 â†” SDK bridge for standard fleet control
**Status:** Production-ready
**Dependencies:**
- ROS2 (`geometry_msgs.Twist`, `sensor_msgs.JointState`, `nav_msgs.Odometry`)
- `booster_robotics_sdk_python`

**Key Features:**
- Standard `/cmd_vel` interface (Nav2 compatible)
- Mode control via `/mode_cmd` topic
- Per-robot namespacing (`/k1_001/`, `/k1_002/`)
- Safety watchdog (1s timeout â†’ auto-stop)
- Status publishing at 10 Hz

**ROS2 Topics:**
- **Subscribe:** `/{robot_id}/cmd_vel` (Twist), `/{robot_id}/mode_cmd` (String)
- **Publish:** `/{robot_id}/status`, `/{robot_id}/joint_states`, `/{robot_id}/odom`

**Usage:**
```bash
python src/booster_ros2_bridge.py k1_001
python src/booster_ros2_bridge.py k1_002 127.0.0.1

# Control via ROS2
ros2 topic pub /k1_001/cmd_vel geometry_msgs/Twist "{linear: {x: 0.5}}"
ros2 topic pub /k1_001/mode_cmd std_msgs/String "data: walking"
```

**Architecture:**
```
ROS2 /cmd_vel â†’ booster_ros2_bridge.py â†’ SDK Move() â†’ K1 Hardware
```

---

### `src/fleet_coordinator.py` ğŸ†• PRODUCTION (NEEDS GUN.JS)
**Purpose:** Distributed fleet coordination via Gun.js
**Status:** Production (requires Gun.js relay server)
**Dependencies:**
- ROS2 (`std_msgs.String`, `geometry_msgs.Pose`)
- `requests` (HTTP client for Gun.js)
- Gun.js relay server (external)

**Key Features:**
- Heartbeat publishing (1 Hz) to Gun.js
- Automatic task discovery & claiming
- Fleet monitoring (all robot states)
- Optimistic locking for task atomicity
- Decentralized coordination (no SPOF)

**ROS2 Topics:**
- **Subscribe:** `/{robot_id}/status`
- **Publish:** `/{robot_id}/coord_cmd`

**Gun.js Database Schema:**
```
/fleet/robots/{robot_id}:
  {
    robot_id: "k1_001",
    battery: 85.0,
    mode: "walking",
    status: "busy",
    position: {x, y, z},
    current_task: "task_123",
    timestamp: 1234567890
  }

/fleet/tasks/{task_id}:
  {
    task_id: "task_123",
    status: "in_progress",
    claimed_by: "k1_001",
    claimed_at: 1234567890,
    description: "Navigate to waypoint A"
  }
```

**Usage:**
```bash
python src/fleet_coordinator.py k1_001 http://gun-relay:8765
```

**âš ï¸ REQUIRES:** Gun.js relay server (see GUN_RELAY_SETUP.md)

---

## ğŸ¤ Support Modules

### `src/tts_module.py` âœ… LIBRARY
**Purpose:** Text-to-speech abstraction (Piper/espeak/pyttsx3)
**Status:** Production with auto-detection
**Dependencies:**
- `subprocess` (for Piper/espeak)
- `pyttsx3` (optional fallback)

**API:**
```python
tts = TextToSpeech(engine='auto')  # Auto-detects: piper â†’ espeak â†’ pyttsx3
tts.speak("Hello world", blocking=True)
```

**Features:**
- Auto-detection of available TTS engines
- Command injection protection
- Timeout protection (10s)

---

### `src/voice_listener.py` âœ… LIBRARY
**Purpose:** Voice command recognition
**Status:** Production (optional dependency)
**Dependencies:**
- `speech_recognition` (optional)
- `pyaudio` (optional)

**API:**
```python
listener = VoiceListener()
listener.start(callback_function)
text = listener.listen_once()
listener.stop()
```

---

## ğŸ§ª Testing & Deployment

### `k1_test_runner.py` âœ… PRODUCTION
**Purpose:** Automated SSH-based testing framework
**Status:** Production
**Dependencies:**
- `subprocess`, `sshpass`

**Tests:**
- Phase 1: Network connectivity (ping, SSH)
- Phase 2: Dependencies (Python, ROS2, camera topics)
- Phase 3: Basic controls connection
- Phase 4: Emergency stop verification

**Usage:**
```bash
python k1_test_runner.py  # Interactive test suite
```

---

### `k1_safe_test.py` âœ… PRODUCTION
**Purpose:** Safe mode transition testing (Damping â†’ Prepare â†’ Walking)
**Status:** Production with safety prompts
**Dependencies:**
- `subprocess`, `sshpass`

**Features:**
- Mode verification
- Safe mode transitions
- Emergency stop testing

---

### `run_k1_tests.sh` âœ… PRODUCTION
**Purpose:** Bash-based test suite
**Status:** Production
**Dependencies:**
- `sshpass`, SSH access to robot

**Tests:**
- Phases 1-6 (connectivity, dependencies, controls, camera, TTS, recognition)

**Usage:**
```bash
./run_k1_tests.sh
```

---

### `deploy_to_k1.sh` âœ… PRODUCTION
**Purpose:** Deploy code to robot via SSH
**Status:** Production
**Dependencies:**
- `sshpass`, `scp`

**Usage:**
```bash
./deploy_to_k1.sh
```

**Copies:**
- All `src/*.py` files
- All `*.sh`, `*.md`, `*.txt` files

---

## ğŸ”§ Utilities

### `convert_yolo_to_tensorrt.sh` ğŸ†• PRODUCTION
**Purpose:** Convert YOLO .pt â†’ .engine for Jetson optimization
**Status:** Production (Jetson-specific)
**Dependencies:**
- `ultralytics` (YOLO export)
- TensorRT (Jetpack)

**Usage:**
```bash
./convert_yolo_to_tensorrt.sh
# Creates: yolov8n.engine, yolov8s.engine, yolov8m.engine
```

**Performance:** 3-7x faster inference with FP16 precision

---

### `install_piper.sh` âœ… PRODUCTION
**Purpose:** Install Piper TTS on Jetson
**Status:** Production (ARM64-optimized)
**Dependencies:**
- Piper TTS binaries

**Usage:**
```bash
./install_piper.sh
```

---

## ğŸ“š Documentation

### `README.md` âœ… COMPLETE
**Purpose:** User-facing documentation
**Content:**
- Quick start guide
- Program descriptions
- Feature list
- Fleet deployment instructions

---

### `SAFETY_CHECKLIST.md` âœ… CRITICAL
**Purpose:** Hardware safety procedures
**Content:**
- Pre-flight checklist (Phases 1-7)
- Emergency procedures
- Known limitations
- Test result log

**CRITICAL:** Read before first hardware test!

---

### `JETSON_OPTIMIZATION.md` âœ… REFERENCE
**Purpose:** Jetson performance optimization guide
**Content:**
- TensorRT setup
- Frame skip strategies
- Resolution tuning
- GPU monitoring

---

### `TTS_OPTIONS.md` âœ… REFERENCE
**Purpose:** Text-to-speech engine comparison
**Content:**
- Piper vs espeak vs pyttsx3
- Installation guides
- Performance notes

---

### `ROADMAP.md` âœ… PLANNING
**Purpose:** Future feature planning
**Content:**
- Planned features
- Timeline estimates
- Integration notes

---

## ğŸ“¦ Configuration

### `requirements.txt` âœ… COMPLETE
**Purpose:** Python dependencies
**Content:**
```
rclpy
cv_bridge
opencv-python
numpy
ultralytics
requests
deepface
SpeechRecognition
pyaudio
pyttsx3
```

**Install:**
```bash
pip install -r requirements.txt
```

---

## ğŸ”— Dependency Graph

```
basic_controls.py
  â””â”€â”€ booster_robotics_sdk_python

basic_cam.py
  â””â”€â”€ ROS2 (rclpy, cv_bridge)

came_yolo.py (3D DEPTH)
  â”œâ”€â”€ ROS2 (rclpy, cv_bridge)
  â”œâ”€â”€ ultralytics (YOLO)
  â””â”€â”€ depth_raw topic (NEW)

cam_face_recognition.py
  â”œâ”€â”€ ROS2
  â”œâ”€â”€ face_recognition.py
  â””â”€â”€ tts_module.py

smart_recognition.py
  â”œâ”€â”€ ROS2
  â”œâ”€â”€ face_recognition.py
  â”œâ”€â”€ tts_module.py
  â”œâ”€â”€ voice_listener.py
  â””â”€â”€ ultralytics (YOLO)

booster_ros2_bridge.py (NEW)
  â”œâ”€â”€ ROS2 (geometry_msgs, sensor_msgs, nav_msgs)
  â””â”€â”€ booster_robotics_sdk_python

fleet_coordinator.py (NEW)
  â”œâ”€â”€ ROS2 (std_msgs, geometry_msgs)
  â”œâ”€â”€ requests (HTTP client)
  â””â”€â”€ Gun.js relay server (EXTERNAL)

face_recognition.py
  â”œâ”€â”€ deepface (optional)
  â””â”€â”€ opencv

tts_module.py
  â”œâ”€â”€ piper (optional)
  â”œâ”€â”€ espeak (optional)
  â””â”€â”€ pyttsx3 (optional)

voice_listener.py
  â”œâ”€â”€ speech_recognition (optional)
  â””â”€â”€ pyaudio (optional)
```

---

## ğŸš€ Deployment Architecture

### Single Robot
```
Jetson Orin NX
  â”œâ”€â”€ K1 SDK Daemon (localhost:127.0.0.1)
  â”œâ”€â”€ ROS2 Humble
  â”œâ”€â”€ basic_controls.py (SDK control)
  â”œâ”€â”€ came_yolo.py (3D vision)
  â””â”€â”€ smart_recognition.py (AI interaction)
```

### Multi-Robot Fleet
```
Gun.js Relay Server (http://gun-relay:8765)
  â†“
Fleet Database (/fleet/robots/, /fleet/tasks/)
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Robot 001  â”‚  Robot 002  â”‚  Robot 003  â”‚
â”‚             â”‚             â”‚             â”‚
â”‚ booster_ros2â”‚ booster_ros2â”‚ booster_ros2â”‚
â”‚  _bridge.py â”‚  _bridge.py â”‚  _bridge.py â”‚
â”‚      â†“      â”‚      â†“      â”‚      â†“      â”‚
â”‚ fleet_coord â”‚ fleet_coord â”‚ fleet_coord â”‚
â”‚  inator.py  â”‚  inator.py  â”‚  inator.py  â”‚
â”‚      â†“      â”‚      â†“      â”‚      â†“      â”‚
â”‚  K1 SDK     â”‚  K1 SDK     â”‚  K1 SDK     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Status Summary

| Component | Status | Notes |
|-----------|--------|-------|
| SDK Control | âœ… Production | 127.0.0.1 fix applied |
| Camera Feed | âœ… Production | NV12 support |
| 3D Vision | ğŸ†• New | Depth integration added |
| TensorRT | ğŸ†• New | 3-7x speedup |
| ROS2 Bridge | ğŸ†• New | Fleet-ready |
| Fleet Coord | ğŸ†• New | Needs Gun.js server |
| Safety System | âœ… Production | Emergency stop tested |
| Documentation | âœ… Complete | All guides updated |

**Next Steps:**
1. âœ… Set up Gun.js relay server (see GUN_RELAY_SETUP.md)
2. Test multi-robot coordination
3. Deploy Nav2 navigation stack
4. Implement wave detection greeter bot

---

**For Fleet Deployment:** See GUN_RELAY_SETUP.md
**For Safety:** Read SAFETY_CHECKLIST.md first!
**For Performance:** See JETSON_OPTIMIZATION.md
